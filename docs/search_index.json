[["index.html", "Analisis de Dow Jone Index para materia Analisis de series de tiempo 1 Introducción", " Analisis de Dow Jone Index para materia Analisis de series de tiempo Paula López 2024-05-01 1 Introducción Para el desarollo de esta actividad se elije los datos: Dow Jones Index Justificación: El índice Dow Jones es uno de los indicadores bursátiles más reconocidos y utilizados para medir el rendimiento de las acciones de grandes empresas estadounidenses. información sobre 30 compañías que forman parte de este índice, incluyendo precios de apertura y cierre, volumen de transacciones, precios máximos y mínimos, entre otros. Trabajar con estos datos a lo largo del curso nos permitirá desarrollar habilidades analíticas y comprender mejor la dinámica de los mercados financieros, una de las ramas donde mas se buscan a científicos de datos. Fuente: La base de datos que utilizaremos proviene del UCI Machine Learning Repository, que ofrece acceso público a datos con fines académicos y de investigación. No hay restricciones para su uso. https://archive.ics.uci.edu/dataset/312/dow+jones+index Característica de los datos: Tipo: Time-Series Tareas asociadas: Classification, Clustering N Instancias: 750 N Caracteres: 15 Fecha de creacion de data set: 10/22/2014 . "],["exploracion-de-datos-eda.html", "2 Exploracion de datos: EDA", " 2 Exploracion de datos: EDA Iniciamos la actividad con cargue de datos, limpieza y comprensión para saber cómo abordar el análisis. #cargue del data set folder_path &lt;- &quot;C:/Users/e184385/OneDrive - WFT/Desktop/Msc/Semestre 2/Analisis de series de tiempo/DJI&quot; file_path &lt;- file.path(folder_path, &quot;dow_jones_index.data&quot;) # Leer el archivo CSV my_data &lt;- read.csv(file_path, header = TRUE, sep = &quot;,&quot;) # `sep` para indicar que es CSV Manejo de columnas con caracteres especiales: el archivo tiene columnas con valores monetarios (como $15.82). Para evitar problemas al trabajar con estos datos, se elimina el símbolo $ y se convierte los valores a numéricos: # Convertir columnas de caracteres a numéricos numeric_columns &lt;- c(&quot;open&quot;, &quot;high&quot;, &quot;low&quot;, &quot;close&quot;, &quot;next_weeks_open&quot;, &quot;next_weeks_close&quot;) # Las columnas que contienen precios # Remover el símbolo `$` y convertir a numérico for (col in numeric_columns) { my_data[[col]] &lt;- as.numeric(gsub(&quot;\\\\$&quot;, &quot;&quot;, my_data[[col]])) } Verificación de contenido: después de cargar el archivo y convertir las columnas necesarias, se verifica el contenido para entender la base de datos # Mostrar la estructura del data frame str(my_data) ## &#39;data.frame&#39;: 750 obs. of 16 variables: ## $ quarter : int 1 1 1 1 1 1 1 1 1 1 ... ## $ stock : chr &quot;AA&quot; &quot;AA&quot; &quot;AA&quot; &quot;AA&quot; ... ## $ date : chr &quot;1/7/2011&quot; &quot;1/14/2011&quot; &quot;1/21/2011&quot; &quot;1/28/2011&quot; ... ## $ open : num 15.8 16.7 16.2 15.9 16.2 ... ## $ high : num 16.7 16.7 16.4 16.6 17.4 ... ## $ low : num 15.8 15.6 15.6 15.8 16.2 ... ## $ close : num 16.4 16 15.8 16.1 17.1 ... ## $ volume : int 239655616 242963398 138428495 151379173 154387761 114691279 80023895 132981863 109493077 114332562 ... ## $ percent_change_price : num 3.79 -4.43 -2.47 1.64 5.93 ... ## $ percent_change_volume_over_last_wk: num NA 1.38 -43.02 9.36 1.99 ... ## $ previous_weeks_volume : int NA 239655616 242963398 138428495 151379173 154387761 114691279 80023895 132981863 109493077 ... ## $ next_weeks_open : num 16.7 16.2 15.9 16.2 17.3 ... ## $ next_weeks_close : num 16 15.8 16.1 17.1 17.4 ... ## $ percent_change_next_weeks_price : num -4.428 -2.471 1.638 5.933 0.231 ... ## $ days_to_next_dividend : int 26 19 12 5 97 90 83 76 69 62 ... ## $ percent_return_next_dividend : num 0.183 0.188 0.19 0.186 0.175 ... Durante el análisis se va a revisar la variación de diferentes acciones (stocks) en el tiempo, revisamos ahora cuantas stocks tenemos en la base de datos # Ver todos los valores únicos en la columna &#39;stock&#39; unique(my_data$stock) ## [1] &quot;AA&quot; &quot;AXP&quot; &quot;BA&quot; &quot;BAC&quot; &quot;CAT&quot; &quot;CSCO&quot; &quot;CVX&quot; &quot;DD&quot; &quot;DIS&quot; &quot;GE&quot; &quot;HD&quot; &quot;HPQ&quot; ## [13] &quot;IBM&quot; &quot;INTC&quot; &quot;JNJ&quot; &quot;JPM&quot; &quot;KRFT&quot; &quot;KO&quot; &quot;MCD&quot; &quot;MMM&quot; &quot;MRK&quot; &quot;MSFT&quot; &quot;PFE&quot; &quot;PG&quot; ## [25] &quot;T&quot; &quot;TRV&quot; &quot;UTX&quot; &quot;VZ&quot; &quot;WMT&quot; &quot;XOM&quot; Para el análisis de serie de tiempo es importante conocer el inicio y final del conjunto de datos. # Obtener la primera y ultima fecha de data frame first_date &lt;- min(my_data$date, na.rm = TRUE) # `na.rm = TRUE` para ignorar NA last_date &lt;- max(my_data$date, na.rm = TRUE) print(first_date) # Primera fecha ## [1] &quot;1/14/2011&quot; print(last_date) # Última fecha ## [1] &quot;6/3/2011&quot; Procedemos a identificar los datos faltantes para cada columna del data set # Contar el número de NA por cada columna na_count &lt;- sapply(my_data, function(x) sum(is.na(x))) print(na_count) # Muestra el número de NA en cada variable ## quarter stock ## 0 0 ## date open ## 0 0 ## high low ## 0 0 ## close volume ## 0 0 ## percent_change_price percent_change_volume_over_last_wk ## 0 30 ## previous_weeks_volume next_weeks_open ## 30 0 ## next_weeks_close percent_change_next_weeks_price ## 0 0 ## days_to_next_dividend percent_return_next_dividend ## 0 0 La base de datos tiene datos faltantes, se procede a imputarlos. # Imputar valores faltantes con la media de la columna my_data$percent_change_volume_over_last_wk[is.na(my_data$percent_change_volume_over_last_wk)] &lt;- mean(my_data$percent_change_volume_over_last_wk, na.rm = TRUE) # Imputar valores faltantes con la mediana de la columna my_data$previous_weeks_volume[is.na(my_data$previous_weeks_volume)] &lt;- median(my_data$previous_weeks_volume, na.rm = TRUE) # Contar el número de NA por columna na_count &lt;- sapply(my_data, function(x) sum(is.na(x))) print(na_count) # Muestra el número de NA en cada variable ## quarter stock ## 0 0 ## date open ## 0 0 ## high low ## 0 0 ## close volume ## 0 0 ## percent_change_price percent_change_volume_over_last_wk ## 0 0 ## previous_weeks_volume next_weeks_open ## 0 0 ## next_weeks_close percent_change_next_weeks_price ## 0 0 ## days_to_next_dividend percent_return_next_dividend ## 0 0 Orden de data frame: Se convierte la columan Date a formato Date para facilitar analisis en R y se ordenan datos por fecha. # Convertir la columna &#39;date&#39; a formato Date my_data$date &lt;- as.Date(my_data$date, format = &quot;%m/%d/%Y&quot;) # ordenar por fecha my_data &lt;- my_data[order(my_data$date), ] Visualizacion de datos library(ggplot2) ggplot(my_data, aes(x = as.Date(date, &quot;%m/%d/%Y&quot;), y = close, color = stock)) + geom_line() + # Gráfico de línea labs(title = &quot;Precio de Cierre por Acción a lo Largo del Tiempo&quot;, x = &quot;Fecha&quot;, y = &quot;Precio de Cierre&quot;) + theme_minimal() + scale_x_date( date_breaks = &quot;1 month&quot;, # Etiquetas cada mes date_labels = &quot;%B&quot; # Mostrar solo el nombre del mes ) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Inclinación de etiquetas # Gráfico de facetas por acción ggplot(my_data, aes(x = as.Date(date, &quot;%m/%d/%Y&quot;), y = close)) + geom_line() + # Gráfico de línea labs(title = &quot;Precio de Cierre por Acción a lo Largo del Tiempo&quot;, x = &quot;Fecha&quot;, y = &quot;Precio de Cierre&quot;) + theme_minimal() + facet_wrap(~ stock) # Facetas para cada acción La base de datos contiene 30 stock, se decie de crear una base de datos que contenga solo una stock ““IBM” y realizar en adelante todos los analisis sobre esta accion. # Crear un nuevo data frame con solo IBM data_ibm &lt;- subset(my_data, stock == &quot;IBM&quot;) # Verificar el nuevo data frame print(unique(data_ibm$stock)) ## [1] &quot;IBM&quot; str(data_ibm) ## &#39;data.frame&#39;: 25 obs. of 16 variables: ## $ quarter : int 1 1 1 1 1 1 1 1 1 1 ... ## $ stock : chr &quot;IBM&quot; &quot;IBM&quot; &quot;IBM&quot; &quot;IBM&quot; ... ## $ date : Date, format: &quot;2011-01-07&quot; &quot;2011-01-14&quot; &quot;2011-01-21&quot; ... ## $ open : num 147 147 150 155 159 ... ## $ high : num 149 150 157 164 164 ... ## $ low : num 147 146 149 155 159 ... ## $ close : num 148 150 156 159 164 ... ## $ volume : int 23492843 15335348 35770931 32510483 25377163 26106753 14352613 21443811 21549118 31119857 ... ## $ percent_change_price : num 0.489 2.041 3.791 2.439 3.028 ... ## $ percent_change_volume_over_last_wk: num 5.59 -34.72 133.26 -9.11 -21.94 ... ## $ previous_weeks_volume : num 52945558 23492843 15335348 35770931 32510483 ... ## $ next_weeks_open : num 147 150 155 159 164 ... ## $ next_weeks_close : num 150 156 159 164 164 ... ## $ percent_change_next_weeks_price : num 2.04 3.79 2.44 3.03 -0.14 ... ## $ days_to_next_dividend : int 32 25 18 11 4 84 77 70 63 56 ... ## $ percent_return_next_dividend : num 0.439 0.433 0.418 0.408 0.396 ... Gráfico de volumen de acción IBM: El volumen es un indicador clave en el análisis de acciones, ya que muestra la cantidad de acciones negociadas durante un período. se grafica el volumen para ver cómo varía con el tiempo: library(ggplot2) # Gráfico de volumen por acción a lo largo del tiempo ggplot(data_ibm, aes(x = as.Date(date, &quot;%m/%d/%Y&quot;), y = volume, color = stock)) + geom_line() + # Gráfico de línea para volumen labs(title = &quot;Volumen de Acciones por Acción a lo Largo del Tiempo&quot;, x = &quot;Fecha&quot;, y = &quot;Volumen&quot;) + theme_minimal() + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%B&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Gráfico de cambios porcentuales de recio La variable:percent_change_price muestra el cambio porcentual en el precio. Graficar esta variable puede proporcionar información sobre la volatilidad y tendencias de los precios de la accion # Gráfico de cambio porcentual de precio por acción ggplot(data_ibm, aes(x = as.Date(date, &quot;%m/%d/%Y&quot;), y = percent_change_price, color = stock)) + geom_line() + # Gráfico de línea para cambios porcentuales labs(title = &quot;Cambio Porcentual del Precio por Acción a lo Largo del Tiempo&quot;, x = &quot;Fecha&quot;, y = &quot;Cambio Porcentual&quot;) + theme_minimal() + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%B&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) "],["creación-de-serie-temporal.html", "3 Creación de serie temporal 3.1 Determinar la frecuencia de la serie temporal 3.2 Crear la serie temporal con frecuencia semanal", " 3 Creación de serie temporal 3.1 Determinar la frecuencia de la serie temporal La frecuencia determina cuántas observaciones ocurren por unidad de tiempo. Para establecer esto, necesitamos saber la distancia entre las fechas y determinar la frecuencia. # Verificar la diferencia entre fechas para determinar la frecuencia date_diff &lt;- diff(data_ibm$date) # Calcular la diferencia entre fechas # Verificar la distribución de los intervalos table(date_diff) ## date_diff ## 6 7 8 ## 1 22 1 # Promedio de la diferencia entre fechas mean(date_diff) # Para ver el intervalo promedio ## Time difference of 7 days 3.2 Crear la serie temporal con frecuencia semanal # Crear una serie temporal a partir de &#39;close&#39; y la fecha de inicio # Frecuencia de 52 semanas por año, para datos semanales ts_data &lt;- ts(data_ibm$close, start = c(2011, 1), frequency = 52) # Verificar la serie temporal print(ts_data) ## Time Series: ## Start = c(2011, 1) ## End = c(2011, 25) ## Frequency = 52 ## [1] 147.93 150.00 155.50 159.21 164.00 163.85 164.84 162.28 161.83 162.43 155.89 162.18 ## [13] 164.27 164.05 166.21 168.28 170.58 168.89 169.92 170.16 167.50 165.05 163.18 164.44 ## [25] 165.07 # Graficar la serie temporal plot(ts_data, type = &quot;l&quot;, main = &quot;Serie Temporal del Precio de Cierre de IBM&quot;, xlab = &quot;Tiempo&quot;, ylab = &quot;Precio de Cierre&quot;) "],["estructura-de-los-datos-en-series-de-tiempo.html", "4 Estructura de los datos en series de tiempo 4.1 Promedio movil 4.2 Rezagos 4.3 Estacionalidad", " 4 Estructura de los datos en series de tiempo 4.1 Promedio movil El promedio móvil ayuda a suavizar las fluctuaciones de corto plazo para identificar tendencias a largo plazo. Existen varias formas de calcular promedios móviles: simples, ponderados y exponenciales. Aquí usaremos un promedio móvil simple con una ventana de 3 períodos (tomar el promedio de los precios de cierre de las últimas tres semanas en cada punto de tiempo). # Instalar y cargar &#39;zoo&#39; if (!requireNamespace(&quot;zoo&quot;, quietly = TRUE)) { install.packages(&quot;zoo&quot;) } suppressPackageStartupMessages(library(zoo)) # Crear un promedio móvil simple con ventana de 3 períodos ts_data_moving_avg &lt;- rollmean(ts_data, k = 3, fill = NA) # Graficar el precio de cierre y el promedio móvil plot(ts_data, type = &quot;l&quot;, col = &quot;blue&quot;, lty = 2, main = &quot;Precio de Cierre con Promedio Móvil&quot;, xlab = &quot;Tiempo&quot;, ylab = &quot;Precio de Cierre&quot;) lines(ts_data_moving_avg, col = &quot;red&quot;, lty = 1) legend(&quot;topleft&quot;, legend = c(&quot;Precio de Cierre&quot;, &quot;Promedio Móvil (3 períodos)&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(2, 1)) 4.2 Rezagos Los rezagos permiten observar cómo un valor anterior afecta a un valor posterior. # Crear un rezago de 1 período ts_data_lag1 &lt;- lag(ts_data, k = 1) # Gráfica para mostrar la relación entre valores actuales y rezagos plot(ts_data_lag1, ts_data, xlab = &quot;Rezago de 1 Período&quot;, ylab = &quot;Precio de Cierre&quot;, main = &quot;Relación entre Precio de Cierre y Rezago de 1 Período&quot;, col = &quot;darkgreen&quot;, pch = 16) # Crear un rezago de 1 período data_ibm$lag_1 &lt;- dplyr::lag(data_ibm$close, n = 1) # Calcular la correlación entre el precio actual y el rezago correlation &lt;- cor(data_ibm$close, data_ibm$lag_1, use = &quot;complete.obs&quot;) print(correlation) ## [1] 0.8706391 Un valor de correlación cercano a 1 indica una relación fuerte y positiva, mientras que un valor cercano a 0 sugiere poca o ninguna relación. Un valor negativo indicaría una relación inversa. Una correlación de 0.87 sugiere que hay un patrón en el tiempo, donde el precio de cierre tiende a moverse de manera similar al valor anterior. Esto puede indicar una tendencia significativa o autocorrelación. 4.3 Estacionalidad La descomposición es un método útil para separar la tendencia, la estacionalidad y el componente aleatorio (ruido). Aquí se intento utilizar la función decompose. # Descomposición de la serie temporal para detectar estacionalidad #decomposed &lt;- decompose(ts_data, type = &quot;multiplicative&quot;) Pero se obtuvo el siguiente error: Error in decompose(): ! time series has no or less than 2 periods Backtrace: 1. stats::decompose(ts_data, type = “multiplicative”) Ejecución interrumpida Mi serie temporal es demasiado corta, no tiene suficientes períodos para un ciclo completo, solo cubre un período.La función decompose() no puede descomponer la serie temporal y no fue posible identificar estacionalidad. "],["preprocesamiento.html", "5 Preprocesamiento 5.1 Estacionariedad 5.2 Diferenciación 5.3 Transformación", " 5 Preprocesamiento 5.1 Estacionariedad Para determinar si la serie es estacionaria, se suelen usar pruebas estadísticas como la prueba de Dickey-Fuller aumentada (ADF, por sus siglas en inglés). Una serie es estacionaria si su media y varianza son constantes en el tiempo y no tiene tendencia. Para evaluar la estacionariedad: # Cargar el paquete tseries para la prueba ADF suppressPackageStartupMessages(library(tseries)) # Realizar la prueba de Dickey-Fuller para evaluar la estacionariedad adf_result &lt;- adf.test(ts_data) print(adf_result) ## ## Augmented Dickey-Fuller Test ## ## data: ts_data ## Dickey-Fuller = -2.6077, Lag order = 2, p-value = 0.3409 ## alternative hypothesis: stationary Los resultados de la prueba de Dickey-Fuller (ADF) indican que la serie temporal no es estacionaria, dado que el p-valor es 0.3409, que es mayor que el nivel de significancia típico de 0.05. Esto significa que no se puede rechazar la hipótesis nula de que la serie tiene una raíz unitaria, que es un indicio de no estacionariedad. Para abordar este problema y transformar la serie en una que sea estacionaria, se suelen emplear algunas técnicas como la diferenciación y las transformaciones logarítmicas. 5.2 Diferenciación La diferenciación es un método común para eliminar la tendencia y estabilizar la varianza en una serie temporal. Consiste en restar el valor anterior del valor actual, creando así una nueva serie que podría ser más estacionaria. # Diferenciar la serie temporal ts_data_diff &lt;- diff(ts_data) # Realizar la prueba ADF en la serie diferenciada adf_result_diff &lt;- adf.test(ts_data_diff) print(adf_result_diff) ## ## Augmented Dickey-Fuller Test ## ## data: ts_data_diff ## Dickey-Fuller = -2.1451, Lag order = 2, p-value = 0.5171 ## alternative hypothesis: stationary El resultado de la prueba de Dickey-Fuller en la serie diferenciada muestra un p-valor de 0.5171, lo que indica que incluso después de la diferenciación, la serie sigue siendo no estacionaria. Esto implica que la serie temporal todavía tiene una raíz unitaria, y por lo tanto, no cumple con las condiciones de estacionariedad. Cuando la diferenciación simple no logra hacer que una serie temporal sea estacionaria,se puede emplear transformaciones. 5.3 Transformación as transformaciones logarítmicas pueden ayudar a estabilizar la varianza y reducir la tendencia en series temporales con crecimiento exponencial o gran variabilidad. Una combinación de transformación logarítmica y diferenciación puede ser eficaz. # Aplicar la transformación logarítmica ts_data_log &lt;- log(ts_data) # Diferenciar la serie logarítmica ts_data_log_diff &lt;- diff(ts_data_log) # Realizar la prueba ADF en la serie logarítmica diferenciada adf_result_log_diff &lt;- adf.test(ts_data_log_diff) print(adf_result_log_diff) ## ## Augmented Dickey-Fuller Test ## ## data: ts_data_log_diff ## Dickey-Fuller = -2.1652, Lag order = 2, p-value = 0.5095 ## alternative hypothesis: stationary La transformación logarítmica y la diferenciación no lograron convertir la serie temporal en estacionaria, según el resultado de la prueba de Dickey-Fuller (ADF). Un p-valor de 0.5095 es alto, lo que indica que la serie sigue siendo no estacionaria incluso después de aplicar estas transformaciones. Cuando una serie temporal no se vuelve estacionaria mediante transformaciones y diferenciaciones simples, es posible que haya factores subyacentes que influyen en la no estacionariedad. procedemos a utlizar otro enfoque para analizarla como el modelo ARIMA. "],["modelo-arima.html", "6 Modelo ARIMA 6.1 Identificación del modelo ARIMA 6.2 Ajuste del modelo ARIMA 6.3 Diagnóstico del modelo ARIMA 6.4 Pronostico del Modelo ARIMA", " 6 Modelo ARIMA 6.1 Identificación del modelo ARIMA El proceso de identificación generalmente comienza con el análisis de autocorrelación (ACF) y autocorrelación parcial (PACF). Esto ayuda a decidir qué valores de p y q son apropiados para el modelo. # Instalar los paquetes necesarios si aún no están instalados if(!require(forecast)) install.packages(&quot;forecast&quot;) if(!require(tseries)) install.packages(&quot;tseries&quot;) # Cargar los paquetes library(forecast) library(tseries) # Análisis de autocorrelación para la serie diferenciada acf(ts_data_log_diff, main = &quot;Autocorrelación de Serie Logarítmica Diferenciada&quot;) # Análisis de autocorrelación parcial para la serie diferenciada pacf(ts_data_log_diff, main = &quot;Autocorrelación Parcial de Serie Logarítmica Diferenciada&quot;) 6.2 Ajuste del modelo ARIMA # Instalar y cargar el paquete forecast si no está instalado if(!require(forecast)) install.packages(&quot;forecast&quot;) # Cargar la serie temporal con transformación logarítmica y diferenciación # (ten en cuenta que ts_data_log_diff es la serie logarítmica diferenciada) ts_data_log &lt;- log(ts_data) # Serie temporal transformada ts_data_log_diff &lt;- diff(ts_data_log) # Serie diferenciada # Utilizar auto.arima para encontrar el mejor modelo ARIMA arima_model &lt;- auto.arima(ts_data_log_diff, trace = TRUE) ## ## Fitting models using approximations to speed things up... ## ## ARIMA(2,0,2) with non-zero mean : Inf ## ARIMA(0,0,0) with non-zero mean : -121.1424 ## ARIMA(1,0,0) with non-zero mean : -117.9833 ## ARIMA(0,0,1) with non-zero mean : -118.6675 ## ARIMA(0,0,0) with zero mean : -121.9732 ## ARIMA(1,0,1) with non-zero mean : Inf ## ## Now re-fitting the best model(s) without approximations... ## ## ARIMA(0,0,0) with zero mean : -121.9732 ## ## Best model: ARIMA(0,0,0) with zero mean # Mostrar detalles del modelo ARIMA seleccionado print(arima_model) ## Series: ts_data_log_diff ## ARIMA(0,0,0) with zero mean ## ## sigma^2 = 0.0003318: log likelihood = 62.08 ## AIC=-122.15 AICc=-121.97 BIC=-120.98 El mejor modelo ARIMA identificado por auto.arima() para tu serie temporal es un ARIMA(0,0,0) con media cero. Este tipo de modelo se conoce como un “modelo de ruido blanco”. Media cero significa que la serie temporal no muestra ninguna dependencia temporal significativa. Este tipo de serie tiene varianza constante y ningún componente de tendencia o estacionalidad detectable. 6.3 Diagnóstico del modelo ARIMA Si el modelo ARIMA que obtuviste es ARIMA(0,0,0) con media cero, esto generalmente sugiere que no hay estructura significativa para modelar, lo que significa que la serie temporal se comporta como ruido blanco. En este caso, el diagnóstico y el pronóstico podrían no proporcionar mucha información adicional porque no hay patrones significativos en la serie temporal. Sin embargo, para completar el proceso de análisis del modelo ARIMA y confirmar que el modelo es adecuado, realizamos el diagnóstico y el pronóstico. Esto te ayudará a verificar si los residuos del modelo son efectivamente ruido blanco y permitirá observar el resultado del pronóstico. El diagnóstico del modelo implica revisar los residuos para asegurarse de que no haya patrones significativos y que se asemejen a ruido blanco. # Diagnóstico del modelo ARIMA: revisión de residuos checkresiduals(arima_model) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,0,0) with zero mean ## Q* = 2.7538, df = 5, p-value = 0.7379 ## ## Model df: 0. Total lags used: 5 El resultado del test de Ljung-Box sugiere que los residuos del modelo ARIMA(0,0,0) con media cero son consistentes con ruido blanco. El p-valor de 0.7379 es alto, indicando que no hay evidencia de autocorrelación significativa en los residuos. Esto es una señal de que el modelo ARIMA ajustado es adecuado y no deja patrones residuales sin modelar. El resultado del test de Ljung-Box, junto con el modelo ARIMA(0,0,0) con media cero, sugiere que no hay patrones significativos o estructura predecible en la serie temporal. Esto indica que la serie temporal actúa como ruido blanco, con variabilidad aleatoria y sin tendencias claras. 6.4 Pronostico del Modelo ARIMA A pesar de que el modelo no muestra patrones predecibles, todavía puedes realizar un pronóstico para comprender cómo se comportará la serie en el futuro. El pronóstico de un modelo ARIMA(0,0,0) con media cero generalmente muestra valores cercanos a cero o variabilidad aleatoria. # Hacer pronóstico para los próximos períodos forecast_result &lt;- forecast(arima_model, h = 10) # Pronóstico para 10 períodos futuros # Visualizar el pronóstico plot(forecast_result, main = &quot;Pronóstico con Modelo ARIMA(0,0,0)&quot;) "],["holter_winter.html", "7 Holter_Winter 7.1 Aplicación el modelo Holt-Winters 7.2 Suavizamiento Exponencial Simple", " 7 Holter_Winter 7.1 Aplicación el modelo Holt-Winters Dado que la serie temporal es demasiado corta para identificar estacionalidad y no se pudo hacer la descomposición, procederemos a aplicar la metodología de Holt-Winters sin componente estacional y Aplicar el suavizamiento exponencial simple (SES) como una alternativa. # Aplicar el modelo Holt-Winters sin componente estacional hw_model &lt;- HoltWinters(ts_data, gamma = FALSE) # Ver el resumen del modelo Holt-Winters summary(hw_model) ## Length Class Mode ## fitted 69 mts numeric ## x 25 ts numeric ## alpha 1 -none- numeric ## beta 1 -none- numeric ## gamma 1 -none- logical ## coefficients 2 -none- numeric ## seasonal 1 -none- character ## SSE 1 -none- numeric ## call 3 -none- call # Graficar los componentes del modelo Holt-Winters sin estacionalidad plot(hw_model) # Diagnóstico del modelo Holt-Winters: revisión de residuos checkresiduals(hw_model) ## ## Ljung-Box test ## ## data: Residuals from HoltWinters ## Q* = 2.1248, df = 5, p-value = 0.8316 ## ## Model df: 0. Total lags used: 5 # Realizar el pronóstico para los próximos 10 períodos hw_forecast &lt;- forecast(hw_model, h = 10) # Visualizar el pronóstico plot(hw_forecast) El modelo Holt-Winters sin componente estacional se ajusta bien a los datos históricos, reflejado en la estrecha correspondencia entre la serie temporal original y los valores ajustados. Los parámetros de suavizado (alpha y beta) son cruciales para modelar la evolución temporal. La prueba de Ljung-Box indica que los residuos no tienen autocorrelación significativa (p = 0.8316), sugiriendo que el modelo captura la variabilidad de manera efectiva. El pronóstico a 10 períodos permite anticipar el comportamiento futuro de los precios de cierre de IBM, respaldado por un SSE bajo que indica un ajuste preciso del modelo a los datos observados. / 7.2 Suavizamiento Exponencial Simple # Aplicar el suavizamiento exponencial simple ses_model &lt;- ses(ts_data, h = 10) # Visualizar el suavizamiento y el pronóstico plot(ses_model) . "],["inclusion-de-variables-en-el-tiempo.html", "8 Inclusion de variables en el tiempo", " 8 Inclusion de variables en el tiempo # Seleccionar la variable de interés (precio de cierre) y &lt;- data_ibm$close # Ajuste del modelo lineal básico lm_model &lt;- lm(y ~ open + high + low + volume, data = data_ibm) # Mostrar un resumen del modelo summary(lm_model) ## ## Call: ## lm(formula = y ~ open + high + low + volume, data = data_ibm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8415 -0.8975 0.1009 1.2151 2.6185 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.226e+01 7.655e+00 1.601 0.1250 ## open -2.295e-01 1.463e-01 -1.569 0.1323 ## high 5.930e-01 2.138e-01 2.773 0.0117 * ## low 5.714e-01 2.372e-01 2.410 0.0257 * ## volume -5.377e-08 7.505e-08 -0.716 0.4820 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.329 on 20 degrees of freedom ## Multiple R-squared: 0.9552, Adjusted R-squared: 0.9462 ## F-statistic: 106.5 on 4 and 20 DF, p-value: 3.468e-13 # Diagnóstico del modelo lineal: revisión de residuos plot(lm_model, which = 1) # Residuos vs valores ajustados plot(lm_model, which = 2) # QQ plot de los residuos # Hacer predicciones con el modelo lineal predictions &lt;- predict(lm_model, newdata = data_ibm) # Visualizar las predicciones si es relevante plot(data_ibm$date, y, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylim = range(y, predictions), main = &quot;Predicciones del Modelo Lineal&quot;, xlab = &quot;Fecha&quot;, ylab = &quot;Precio de Cierre&quot;) lines(data_ibm$date, predictions, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Observado&quot;, &quot;Predicción&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 2) Coeficientes de Regresión El intercepto del modelo es 12.26, lo que sugiere el valor esperado del precio de cierre cuando todas las variables independientes son cero. El coeficiente para ‘open’ es −0.2295, lo que indica que un aumento en el precio de apertura está asociado con una ligera disminución en el precio de cierre, aunque no es estadísticamente significativo. ‘high’ tiene un coeficiente de 0.5930, lo que indica que un aumento en el precio máximo diario está asociado con un aumento en el precio de cierre. Este efecto es estadísticamente significativo (valor p = 0.0117). ‘low’ tiene un coeficiente de 0.5714, lo que indica que un aumento en el precio mínimo diario está asociado con un aumento en el precio de cierre. Este efecto también es estadísticamente significativo (valor p = 0.0257). ‘volume’ muestra un coeficiente muy pequeño (−5.377×10−8−), lo que sugiere que el volumen de operaciones no tiene un efecto significativo en el precio de cierre (valor p = 0.4820). Bondad de Ajuste El modelo explica aproximadamente el 94.62% de la variabilidad en el precio de cierre de IBM, como se indica por el R2 ajustado de 0.9462. Esto sugiere que las variables incluidas (open, high, low, volume) son capaces de explicar una gran parte de la variación en el precio de cierre observado. Significación Estadística ‘high’ y ‘low’ son las variables que muestran significación estadística en relación con el precio de cierre. ‘high’ tiene un efecto positivo significativo, mientras que ‘low’ tiene un efecto positivo pero ligeramente menor en magnitud. ‘open’ y ‘volume’, aunque presentes en el modelo, no tienen un efecto estadísticamente significativo en el precio de cierre. Diagnóstico del Modelo Los residuos del modelo parecen razonables, con una distribución que no muestra patrones claros de no linealidad ni heterocedasticidad (variación no constante de los errores). . "],["modelo-facebooks-prophet.html", "9 Modelo Facebook’s Prophet 9.1 Preparación de la serie temporal 9.2 Ajuste del modelo Prophet", " 9 Modelo Facebook’s Prophet 9.1 Preparación de la serie temporal # Convertir la serie temporal a un data frame para Prophet ts_data_prophet &lt;- data.frame( ds = time(ts_data), # Columna de fechas y = as.numeric(ts_data) # Columna de valores numéricos (precio de cierre) ) 9.2 Ajuste del modelo Prophet # Inicializar un nuevo modelo Prophet m &lt;- prophet() # Ajustar el modelo Prophet fit &lt;- fit.prophet(m, ts_data_prophet) ## Disabling yearly seasonality. Run prophet with yearly.seasonality=TRUE to override this. ## Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. ## Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. ## n.changepoints greater than number of observations. Using 19 Explicación de las advertencias y decisiones automáticas que hace Prophet durante el ajuste del modelo. Disabling yearly, weekly, y daily seasonality: Prophet ha decidido desactivar la estacionalidad anual, semanal y diaria automáticamente para este modelo. Prophet intenta identificar automáticamente patrones estacionales en los datos. Si no detecta suficiente evidencia de estacionalidad en estas frecuencias, las desactiva para evitar ajustes innecesarios que podrían llevar a sobreajustar el modelo. Aunque Prophet ha desactivado las estacionalidades, es posible que los datos de series temporales no muestren patrones claros en estas escalas de tiempo específicas (anual, semanal, diaria). Esto es común en datos financieros donde las estacionalidades pueden ser menos evidentes que en otros tipos de datos. n.changepoints greater than number of observations: Prophet utiliza puntos de cambio para modelar cambios en las tendencias de los datos a lo largo del tiempo. Este mensaje indica que ha detectado más puntos de cambio potenciales de los que hay observaciones en tus datos. En este caso, Prophet ha decidido utilizar 19 puntos de cambio. . "],["conclusiones-y-comentarios.html", "10 Conclusiones y comentarios 10.1 Preprocesamiento de datos 10.2 Creacion de serie temporal 10.3 Estructura de datos en series temporales 10.4 Análisis de modelo ARIMA 10.5 Modelo Holt-Winters 10.6 Modelo Facebook’s Prophet", " 10 Conclusiones y comentarios A lo largo de este análisis, hemos seguido varios pasos para trabajar con series temporales. Aquí están las conclusiones derivadas de nuestro análisis: 10.1 Preprocesamiento de datos Carga y limpieza: Se cargaron los datos, se verificaron valores faltantes (NA), se aplicaron métodos para imputar datos faltantes si era necesario, se selecciono una solo stock para todo el analisis. Formateo de fechas: Se aseguró que las fechas estuvieran en el formato correcto y se organizó el dataset por fecha para garantizar una secuencia temporal adecuada. Visualización de la serie temporal: Se realizaron gráficos para visualizar el comportamiento de las variables, identificando posibles tendencias y patrones. 10.2 Creacion de serie temporal Frecuencia de la serie temporal: se calculó la diferencia entre fechas en la serie temporal. El análisis mostró que la mayoría de los intervalos entre fechas era de 7 días, lo que sugiere una frecuencia semanal. Este dato fue utilizado para establecer una frecuencia de 52 semanas por año al crear la serie temporal. Construcción de la serie temporal: Con base en la frecuencia identificada, se creó una serie temporal utilizando la columna ‘close’ y la fecha de inicio correspondiente. El resultado fue una serie temporal con datos semanales, proporcionando la base para el análisis posterior. 10.3 Estructura de datos en series temporales Promedio móvil: Se utilizó un promedio móvil simple con una ventana de 3 períodos para suavizar las fluctuaciones a corto plazo y revelar tendencias a largo plazo. El gráfico del promedio móvil mostró cómo el suavizado ayuda a visualizar la tendencia en la serie temporal, permitiendo identificar patrones generales. Rezagos y correlación: Se exploró el uso de rezagos para identificar relaciones entre valores anteriores y actuales. La correlación entre el precio actual y el rezago de 1 período fue de 0.8706391, indicando una fuerte relación positiva. Esto sugiere que los valores anteriores tienen un impacto significativo en los valores posteriores, lo cual puede ser útil para la predicción. Estacionalidad y descomposición: Se intentó usar la función decompose para detectar estacionalidad, pero la serie temporal era demasiado corta para un ciclo completo, lo que resultó en un error. Esto implica que no fue posible identificar estacionalidad con la serie temporal existente. Estacionariedad y descomposicion, prueba de Dickey-Fuller: Esta prueba mostró que la serie temporal original no era estacionaria (p-valor &gt; 0.05), lo que indica que había una tendencia o raíz unitaria. Diferenciación y transformación logarítmica: Se aplicaron estas técnicas para estabilizar la serie temporal y eliminar la tendencia. Sin embargo, incluso después de estas transformaciones, la prueba de Dickey-Fuller seguía mostrando no estacionariedad. 10.4 Análisis de modelo ARIMA Identificación del mejor modelo ARIMA: Se utilizó auto.arima() para identificar el modelo ARIMA más adecuado para la serie temporal. El resultado fue ARIMA(0,0,0) con media cero, un modelo de ruido blanco. Diagnóstico del modelo ARIMA: El test de Ljung-Box mostró que los residuos del modelo ARIMA(0,0,0) con media cero eran consistentes con ruido blanco, lo que indica que no había autocorrelaciones significativas. Pronóstico con modelo ARIMA: El pronóstico para 10 períodos futuros con este modelo reflejó la naturaleza de ruido blanco de la serie temporal, mostrando valores sin un patrón claro o predecible. 10.5 Modelo Holt-Winters Modelo Holt-Winters sin componente estacional: El modelo ajusta bien los datos históricos, mostrando que los valores ajustados siguen la tendencia observada. Parámetros de Suavizado: Los parámetros alpha y beta son fundamentales para modelar la evolución temporal de los datos. Diagnóstico de Residuos: La prueba de Ljung-Box mostró un p-valor alto (0.8316), indicando que los residuos no tienen autocorrelación significativa, validando la captura de variabilidad por parte del modelo. Pronóstico: Las predicciones a 10 períodos ofrecen una visión del comportamiento futuro de los precios de cierre de IBM, basado en el patrón histórico. SSE (Sum of Squared Errors): Un SSE bajo indica que el modelo Holt-Winters ajustado correctamente captura la variabilidad de los datos observados. . 10.6 Modelo Facebook’s Prophet Disabling yearly, weekly, y daily seasonality: Prophet ha decidido desactivar la estacionalidad anual, semanal y diaria automáticamente para este modelo. Prophet intenta identificar automáticamente patrones estacionales en los datos. Si no detecta suficiente evidencia de estacionalidad en estas frecuencias, las desactiva para evitar ajustes innecesarios que podrían llevar a sobreajustar el modelo. Aunque Prophet ha desactivado las estacionalidades, es posible que los datos de series temporales no muestren patrones claros en estas escalas de tiempo específicas (anual, semanal, diaria). Esto es común en datos financieros donde las estacionalidades pueden ser menos evidentes que en otros tipos de datos. n.changepoints greater than number of observations: Prophet utiliza puntos de cambio para modelar cambios en las tendencias de los datos a lo largo del tiempo. Este mensaje indica que ha detectado más puntos de cambio potenciales de los que hay observaciones en tus datos. En este caso, Prophet ha decidido utilizar 19 puntos de cambio. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
